{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB Movie Reviews\n",
    "\n",
    "In this lab we will be working through a Sentiment Analysis task using the IMDB Movie Review Dataset. This dataset consists of 50K movie reviews that are labeled as either positive or negative based upon their sentiment toward a film. For this, we will be using a simple Logistic Regression model for Binary Classification. This notebook will guide you through the 4 main steps for this task:\n",
    "\n",
    "1. Text Preprocessing\n",
    "2. Feature Engineering\n",
    "3. Model Fitting\n",
    "4. Hyper-Parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Text Preprocessing\n",
    "\n",
    "The first step to this process will be loading the data. The data can be found [here](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews?resource=download). Please download the data from this page which will produce a zip folder. Upon unzipping the folder, a file entitled \"IMDB Dataset.csv\" will be produced. Create a folder to hold data used in this course and place the \"IMDB Dataset.csv\" file in it. Next make sure that every package in the imports below is installed. The \"nltk.download()\" lines only need to be ran once. So comment them out with a # at the beginning of the line after the downloads have been completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/x-dchawra/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/x-dchawra/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/x-dchawra/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, set the **data_path** variable equal to the path of the \"IMDB Dataset.csv\" file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/home/x-dchawra/nlpexperiments/battelle_example/IMDB Dataset.csv.zip\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization and Cleaning\n",
    "\n",
    "In the next two cells, the data will be loaded, cleaned, and normalized. This will include:\n",
    "\n",
    "1. Removing HTML chunks\n",
    "2. Transforming text to lower case\n",
    "3. Tokenizing text\n",
    "4. Removing Stop Words and punctuation\n",
    "5. Lemmatizing Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_path)\n",
    "data.columns = ['text', 'label']\n",
    "data['label'] = data.label.map({'negative': 0, 'positive': 1})\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punct_remover = str.maketrans('', '', punctuation)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub('<.*?>', '', text)\n",
    "    tokens = wordpunct_tokenize(text.lower())\n",
    "    tokens = [i.translate(punct_remover) for i in tokens]\n",
    "    tokens = [i for i in tokens if i not in stop_words]\n",
    "    tokens = [wnl.lemmatize(i) for i in tokens]\n",
    "    text = \" \".join(tokens)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:25<00:00, 1924.14it/s]\n"
     ]
    }
   ],
   "source": [
    "text_data = []\n",
    "for i in tqdm(data.text):\n",
    "    text_data.append(preprocess_text(i))\n",
    "data['text'] = text_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "For features, we will be using a Bag-of-Words model, so each review will be represented as a vector that contains the of each word seen in the corpus that is contained in the review. We can alter the features that are generated by setting the size or range of ngrams that we use to make the vector. An ngram is the number of words per token that is used to make up a vector. For instance, if we wanted to make a bigram, or 2-gram, model, then the vector for each review would consist of the counts of each 2 word phrase. It is also possible to use a mixed-gram model which would use multiple sized ngrams.\n",
    "\n",
    "**Experiment:** Try different sizes and ranges of ngrams and different thresholds for minimum and maximum frequency for feature generate. Observe and note the effects on training time and performance on the test set. The exact variables in question are ngram_min, ngram_max, min_df, and max_df. Please refer to the documentation of CountVectorizer [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html?highlight=countvectorizer#sklearn.feature_extraction.text.CountVectorizer) to determine acceptable values. In short, the 4 values that can be tuned are:\n",
    "\n",
    "* ngram_min - Minimum size of ngrams to use. Acceptable values are integers greater than or equal to 1\n",
    "* ngram_max - Maximum size of ngrams to use. Acceptable values are integers greater than or equal to 1\n",
    "* min_df - Minimum document frequency to be included in model. Acceptable values are integers greater than or equal to 1 to denote the raw count or float values in [0.0, 1.0] to denote percent frequency\n",
    "* max_df - Minimum document frequency to be included in model. Same acceptable values as min_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 0.872\n",
      "0.872\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import threading\n",
    "import numpy as np\n",
    "from threading import Thread\n",
    "import concurrent\n",
    "\n",
    "\n",
    "\n",
    "def modelTester(ngram_min, ngram_max, min_df, max_df):\n",
    "    train_df, test_df = train_test_split(data, test_size=0.2)\n",
    "    vectorizer = CountVectorizer(ngram_range=(\n",
    "        ngram_min, ngram_max), min_df=min_df, max_df=max_df)\n",
    "    x_train = vectorizer.fit_transform(train_df.text)\n",
    "    x_test = vectorizer.transform(test_df.text)\n",
    "\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    term_counts = x_train.toarray().sum(axis=0).tolist()\n",
    "    term_counts = sorted(zip(terms, term_counts), key=lambda x: x[1])\n",
    "    term_counts.reverse()\n",
    "    num_terms = len(terms)\n",
    "\n",
    "    # print(f\"Number of Unique Tokens: {num_terms}\")\n",
    "    # print(\"Most Common Terms:\")\n",
    "    # print(\"-\" * len(\"Most Common Terms:\"))\n",
    "    # for i, j in term_counts[:10]:\n",
    "    #     print(f\"{i} - {j}\")\n",
    "    # print()\n",
    "    # print(\"#\"*20)\n",
    "    # print()\n",
    "\n",
    "    least_common = term_counts[-10:]\n",
    "    least_common.reverse()\n",
    "\n",
    "    # print(\"Least Common Terms:\")\n",
    "    # print(\"-\" * len(\"Least Common Terms:\"))\n",
    "    # for i, j in least_common:\n",
    "    #     print(f\"{i} - {j}\")\n",
    "\n",
    "\n",
    "    model = LogisticRegression(\n",
    "        max_iter=2000, solver='liblinear')\n",
    "    model.fit(x_train.toarray(), train_df.label.values)\n",
    "\n",
    "    test_acc = model.score(x_test, test_df.label.values)\n",
    "    print(f\"Test Set Accuracy: {test_acc}\")\n",
    "\n",
    "    return test_acc\n",
    "\n",
    "# currmax = 0.0\n",
    "# curri = 0.0\n",
    "# for i in np.arange(0.5, 0.001, -0.01):\n",
    "#     print(i)\n",
    "#     acc = modelTester(1, 4, i, 1.0)\n",
    "#     if acc > currmax:\n",
    "#         print(\"newmax\")\n",
    "#         print(acc)\n",
    "#         print(i)\n",
    "#         currmax = acc\n",
    "#         curri = i\n",
    "\n",
    "\n",
    "# print(currmax)\n",
    "print(modelTester(1, 4, 0.006, 1.0))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_min = 1\n",
    "ngram_max = 4\n",
    "min_df = 0.006\n",
    "max_df = 1.0\n",
    "\n",
    "train_df, test_df = train_test_split(data, test_size=0.2)\n",
    "vectorizer = CountVectorizer(ngram_range=(ngram_min, ngram_max), min_df=min_df, max_df=max_df)\n",
    "x_train = vectorizer.fit_transform(train_df.text)\n",
    "x_test = vectorizer.transform(test_df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Unique Tokens: 2932\n",
      "Most Common Terms:\n",
      "------------------\n",
      "movie - 82653\n",
      "film - 74908\n",
      "one - 44232\n",
      "like - 32833\n",
      "time - 24925\n",
      "good - 23974\n",
      "character - 22593\n",
      "story - 20260\n",
      "even - 19775\n",
      "would - 19713\n",
      "\n",
      "####################\n",
      "\n",
      "Least Common Terms:\n",
      "-------------------\n",
      "really make - 244\n",
      "looking forward - 245\n",
      "movie lot - 245\n",
      "really want - 245\n",
      "time watching - 245\n",
      "film actually - 246\n",
      "without doubt - 246\n",
      "basis - 247\n",
      "existent - 247\n",
      "greatly - 247\n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names()\n",
    "term_counts = x_train.toarray().sum(axis=0).tolist()\n",
    "term_counts = sorted(zip(terms, term_counts), key=lambda x: x[1])\n",
    "term_counts.reverse()\n",
    "num_terms = len(terms)\n",
    "\n",
    "print(f\"Number of Unique Tokens: {num_terms}\")\n",
    "print(\"Most Common Terms:\")\n",
    "print(\"-\" * len(\"Most Common Terms:\"))\n",
    "for i, j in term_counts[:10]:\n",
    "    print(f\"{i} - {j}\")\n",
    "print()\n",
    "print(\"#\"*20)\n",
    "print()\n",
    "\n",
    "least_common = term_counts[-10:]\n",
    "least_common.reverse()\n",
    "\n",
    "print(\"Least Common Terms:\")\n",
    "print(\"-\" * len(\"Least Common Terms:\"))\n",
    "for i, j in least_common:\n",
    "    print(f\"{i} - {j}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting Model\n",
    "\n",
    "In this section, we will be fitting the Logistic Regression model. Be aware that the number of tokens used in the Bag-of-Words will determine the size of the model. Including too many features can make the model run for several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Logical Processors: 128\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of Logical Processors: {os.cpu_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above you will see the number of logical processors you computer can use for parallel processing. The more you use the faster your model will train. Setting it to the total number of processors will be the fastest but will also use the most memory. Setting it to the max is my recommendation. The **num_processors** variable below is set to -1 which will tell the model to use every processor. If your model is still taking more than ~5 minutes to train than you likely have included too many tokens in your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/anvil/external/apps/jupyter/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 3.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=2000, n_jobs=-1, solver='liblinear')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_processors = -1\n",
    "\n",
    "model = LogisticRegression(max_iter=2000, n_jobs=num_processors, solver='liblinear')\n",
    "model.fit(x_train.toarray(), train_df.label.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below will be the accuracy of the model on a held-out test set. The maximal score is 1.0 and 0.5 would be equivalent to a random guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 0.8734\n"
     ]
    }
   ],
   "source": [
    "test_acc = model.score(x_test, test_df.label.values)\n",
    "print(f\"Test Set Accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "In the next Markdown cell add notes of your observations. What is the effect of the number of tokens included? How did it impact performance? What size of ngrams and number of tokens produced the best model? Simply using frequency might not be the best method for selecting features, how might you do it differently?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add notes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next two cells to perform hyper-parameter tuning and see the results of the best hyper-parameters on the test set. It is not uncommon for hyper-parameter tuning to take several minutes. If it takes longer than ~10, you may need to interrupt the cell or restart the kernel and try again with a different ngram size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizeModel(parameters):\n",
    "    log_model = LogisticRegression(max_iter=2000, solver='liblinear')\n",
    "    grid = GridSearchCV(log_model, parameters, n_jobs=-1)\n",
    "    grid.fit(x_train, train_df.label.values)\n",
    "\n",
    "    best_model = grid.best_estimator_\n",
    "    print(\"Best_Parameters:\")\n",
    "    for k, v in grid.best_params_.items():\n",
    "        print(f\"{k} = {v}\")\n",
    "    print()\n",
    "    print(\"#\" * 20)\n",
    "    print()\n",
    "\n",
    "    best_test_acc = best_model.score(x_test, test_df.label.values)\n",
    "    print(f\"New Test Accuracy: {best_test_acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best_Parameters:\n",
      "C = 0.1\n",
      "penalty = l1\n",
      "\n",
      "####################\n",
      "\n",
      "New Test Accuracy: 0.8793\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "hyplist = {'penalty': ['l1', 'l2'], 'C': [0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "optimizeModel(hyplist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment:** Test different variables and values for hyper-parameter tuning to see the best results that you can generate. Provide notes in the next cell for your findings in testing with hyper-parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add notes here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ee1cd9b302554fc418f8b84f658dab3188467b37dcaeb92c53502d81e2e402a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
